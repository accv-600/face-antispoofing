{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import brentq\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "from scipy.interpolate import interp1d\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.optim import Adam\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import timm\n",
    "import random\n",
    "import torch.nn.functional as F\n",
    "from scipy.ndimage import gaussian_filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# Define paths to your data folders\n",
    "train_labeled_data_dir = '/data/train/'\n",
    "train_unlabeled_data_dir = '/data/unlabel/'\n",
    "val_data_dir = '/data/validation/'\n",
    "test_data_dir = '/data/test/'\n",
    "\n",
    "\n",
    "# Define transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "# Custom dataset class for semi-supervised learning\n",
    "class SemiSupervisedDataset(Dataset):\n",
    "    def __init__(self, labeled_data_dir, unlabeled_data_dir, transform=None):\n",
    "        self.labeled_dataset = datasets.ImageFolder(root=labeled_data_dir, transform=transform)\n",
    "        self.unlabeled_dataset = datasets.ImageFolder(root=unlabeled_data_dir, transform=transform)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        labeled_image, label = self.labeled_dataset[index % len(self.labeled_dataset)]\n",
    "        unlabeled_image, _ = self.unlabeled_dataset[index % len(self.unlabeled_dataset)]\n",
    "        return labeled_image, label, unlabeled_image\n",
    "\n",
    "    def __len__(self):\n",
    "        return max(len(self.labeled_dataset), len(self.unlabeled_dataset))\n",
    "\n",
    "# Load labeled, unlabeled, validation, and test datasets\n",
    "train_dataset = SemiSupervisedDataset(train_labeled_data_dir, train_unlabeled_data_dir, transform=transform)\n",
    "val_dataset = datasets.ImageFolder(root=val_data_dir, transform=transform)\n",
    "test_dataset = datasets.ImageFolder(root=test_data_dir, transform=transform)\n",
    "\n",
    "# Print the number of images in each dataset\n",
    "print(f\"Number of labeled images in the training set: {len(train_dataset.labeled_dataset)}\")\n",
    "print(f\"Number of unlabeled images in the training set: {len(train_dataset.unlabeled_dataset)}\")\n",
    "print(f\"Number of images in the validation set: {len(val_dataset)}\")\n",
    "print(f\"Number of images in the test set: {len(test_dataset)}\")\n",
    "\n",
    "# Create DataLoader for training, validation, and test sets\n",
    "batch_size = 8  # Adjust according to your needs\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model architecture\n",
    "class SimpleDenseNet(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(SimpleDenseNet, self).__init__()\n",
    "        self.model = timm.create_model('densenet201', pretrained=True)\n",
    "        in_features = self.model.classifier.in_features\n",
    "        self.model.classifier = nn.Sequential(\n",
    "            nn.Linear(in_features, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# Instantiate the custom DenseNet model\n",
    "num_classes = 2  # Binary classification\n",
    "model = SimpleDenseNet(num_classes)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.0001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Gaussian filter to confidence scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Train the model with semi-supervised learning\n",
    "num_epochs = 100\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "early_stopping_patience = 3\n",
    "patience_counter = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for labeled_images, labels, unlabeled_images in train_loader:\n",
    "        labeled_images, labels, unlabeled_images = labeled_images.to(device), labels.to(device), unlabeled_images.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass for labeled images\n",
    "        labeled_outputs = model(labeled_images)\n",
    "        labeled_loss = criterion(labeled_outputs, labels)\n",
    "\n",
    "        # Forward pass for unlabeled images (no labels used)\n",
    "        unlabeled_outputs = model(unlabeled_images)\n",
    "        pseudo_labels = torch.softmax(unlabeled_outputs, dim=1).argmax(dim=1)\n",
    "        pseudo_loss = criterion(unlabeled_outputs, pseudo_labels)\n",
    "        \n",
    "        # Apply Gaussian filter to confidence scores\n",
    "        confidence_scores = torch.max(F.softmax(unlabeled_outputs, dim=1), dim=1)[0]\n",
    "        sigma = 3.0  # Adjust the standard deviation based on your requirements\n",
    "        filtered_confidence_scores = gaussian_filter(confidence_scores.detach().cpu().numpy(), sigma)\n",
    "\n",
    "        # Convert back to torch tensor\n",
    "        filtered_confidence_scores = torch.tensor(filtered_confidence_scores, dtype=torch.float32, device=device)\n",
    "\n",
    "        # Combine losses\n",
    "        weighted_pseudo_loss = pseudo_loss * filtered_confidence_scores\n",
    "        loss = labeled_loss + weighted_pseudo_loss.mean()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Validate the model\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    all_labels = []\n",
    "    all_scores = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            scores = F.softmax(outputs, dim=1)[:, 1]\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_scores.extend(scores.cpu().numpy())\n",
    "\n",
    "    val_loss /= len(val_loader)\n",
    "\n",
    "    # Early stopping check\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= early_stopping_patience:\n",
    "            print(\"Validation loss didn't improve for {} epochs. Early stopping...\".format(early_stopping_patience))\n",
    "            break\n",
    "\n",
    "    # Calculate EER once after training on the validation set\n",
    "    fpr, tpr, thresholds = roc_curve(all_labels, all_scores, pos_label=1)\n",
    "\n",
    "    # Check for NaN values in the arrays\n",
    "    if any(np.isnan(fpr)) or any(np.isnan(tpr)) or any(np.isnan(thresholds)):\n",
    "        print(\"Error: NaN values encountered in fpr, tpr, or thresholds during EER calculation. Skipping this epoch.\")\n",
    "        continue  # Skip to the next epoch\n",
    "    else:\n",
    "        eer = brentq(lambda x: 1. - x - interp1d(fpr, tpr)(x), 0., 1.)\n",
    "        threshold = thresholds[np.nanargmin(np.abs(fpr - eer))]\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, Validation EER: {eer * 100:.2f}%\")\n",
    "        print(f\"Validation EER Threshold: {threshold:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "test_labels = []\n",
    "test_scores = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        scores = torch.nn.functional.softmax(outputs, dim=1)[:, 1]\n",
    "\n",
    "        test_labels.extend(labels.cpu().numpy())\n",
    "        test_scores.extend(scores.cpu().numpy())\n",
    "\n",
    "# Apply Gaussian weighting to test set scores\n",
    "sigma = 3.0  # Adjust the standard deviation based on your requirements\n",
    "weighted_test_scores = gaussian_filter(test_scores, sigma)\n",
    "\n",
    "# Calculate the HTER on the testing set using the EER threshold\n",
    "threshold_test = threshold  # Use the EER threshold from the validation set for testing\n",
    "predicted_labels_test = [1 if score > threshold_test else 0 for score in weighted_test_scores]\n",
    "\n",
    "false_acceptance_test = sum(1 for i in range(len(predicted_labels_test)) if predicted_labels_test[i] == 1 and test_labels[i] == 0)\n",
    "false_rejection_test = sum(1 for i in range(len(predicted_labels_test)) if predicted_labels_test[i] == 0 and test_labels[i] == 1)\n",
    "\n",
    "total_samples_test = len(test_labels)\n",
    "hter_test = ((false_acceptance_test + false_rejection_test) / (2 * total_samples_test)) * 100\n",
    "print(f\"HTER using EER threshold, Gaussian weighting, and L2 regularization: {hter_test:.2f}%\")\n",
    "\n",
    "# Calculate AUC on the test set\n",
    "auc_test = roc_auc_score(test_labels, test_scores)\n",
    "print(f\"Area Under the ROC Curve (AUC) on the test set: {auc_test * 100:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
